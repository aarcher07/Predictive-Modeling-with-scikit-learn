{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Review\n",
    "## 0.A Scikit-Learn\n",
    "\n",
    "Scikit-Learn is a machine learning python package. It allows users to access machine learning algorithms via **object-oriented programming**.\n",
    "\n",
    "## 0.B Data Set\n",
    "\n",
    "I will be using a dataset of avocado prices.\n",
    "\n",
    "We would like to learn prices of avocado given brand, location sold, total demand, etc.\n",
    "\n",
    "## 0.C Load Data\n",
    "\n",
    "Now, we load our training and test set. Run the code below to load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load explanatory variables\n",
    "avocado_training_set  = pd.read_csv(\"datasets/avocado_training_set\",index_col=0)\n",
    "avocado_training_set['year'] = avocado_training_set['year'].astype(str)\n",
    "avocado_training_set['Month'] = avocado_training_set['Month'].astype(str)\n",
    "\n",
    "avocado_test_set = pd.read_csv(\"datasets/avocado_test_set\",index_col=0)\n",
    "avocado_test_set['year'] = avocado_test_set['year'].astype(str)\n",
    "avocado_test_set['Month'] = avocado_test_set['Month'].astype(str)\n",
    "\n",
    "# load predictors\n",
    "prices_training_set = pd.read_csv(\"datasets/avocado_prices_training_set\",index_col=0)\n",
    "prices_test_set = pd.read_csv(\"datasets/avocado_prices_test_set\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we hot-one encoded some categorical variables in the training and test set. In this section, we hot-one encode all categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables_ = ['Month','region','type','year']\n",
    "\n",
    "# split all categorical variables for training set\n",
    "avocado_split_ = pd.get_dummies(data=avocado_training_set[categorical_variables_],\n",
    "                                     drop_first=True)\n",
    "avocado_training_set_cleaned = pd.concat([avocado_training_set,avocado_split_], \n",
    "                                     axis=1, sort=False)\n",
    "avocado_training_set_cleaned = avocado_training_set_cleaned.drop(categorical_variables_,axis=1,\n",
    "                                                         inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables_ = ['Month','region','type','year']\n",
    "\n",
    "# split all categorical variables \n",
    "avocado_split_ = pd.get_dummies(data=avocado_test_set[categorical_variables_],\n",
    "                                     drop_first=True)\n",
    "avocado_test_set_cleaned = pd.concat([avocado_test_set,avocado_split_], \n",
    "                                     axis=1, sort=False)\n",
    "avocado_test_set_cleaned = avocado_test_set_cleaned.drop(categorical_variables_,axis=1,\n",
    "                                                         inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this section, we will learning how to certain the most accurate linear regression model using our training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.A Linear Regression\n",
    "\n",
    "2. Then, train a **machine learning model** using **labeled data**\n",
    "\n",
    "    - \"Labeled data\" has been labeled with the outcome\n",
    "    - \"Machine learning model\" learns the relationship between the attributes of the data and its outcome\n",
    "\n",
    "Linear regression assumes that there is a *linear* relationship between the explanatory variables and the outcome. \n",
    "\n",
    "That our case linear regression means that price does up or down, but not both, at a constant rate if an explanatory variable increases.\n",
    "\n",
    "Thus, linear regression assumes a linear model of price\n",
    "\n",
    "$$\\text{Price} = \\beta_0 + \\beta_1 \\text{Total Volume} + \\beta_2 \\text{Year} + \\beta_3 \\text{Month} + \\beta_4 \\text{Type}.$$\n",
    "\n",
    "In a linear regression model, we aim learn the coefficients, $\\beta_0 ,\\beta_1 ,\\beta_2 ,\\beta_3,\\beta_4$ that minimizes the mean squared error between the model and true response variables (prices). \n",
    "\n",
    "That is,\n",
    "$$\\min_{\\beta_0 ,\\beta_1 ,\\beta_2 ,\\beta_3,\\beta_4} \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\beta_0 + \\beta_1 \\text{Total Volume}_i + \\beta_2 \\text{Year}_i + \\beta_3 \\text{Month}_i + \\beta_4 \\text{Type}_i\\right)^2$$\n",
    "\n",
    "In learning a model, we will be to predict future prices and study how the explanatory variables affect price.\n",
    "    \n",
    "## 10.B Check Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we loaded the correct dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print head of labels_training_set\n",
    "prices_training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print head of  DNA_training_set\n",
    "\n",
    "avocado_training_set_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado_training_set_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.C Statistical Analysis\n",
    "\n",
    "In the avocado dataset, we have many explanatory variables. **How do we pick the explanatory variables that best explain the variation in the prices?**\n",
    "\n",
    "Interestingly, any random explanatory variable (as long as this variable is not colinear with another explanatory variable) will increase the $R^2$ value. Thus, increasing the accuracy of the dataset. However, this increase in accuracy may be insignificant. **How determine explanatory variables meaningfully reduce the error in the model?**\n",
    "\n",
    "Typically, one uses Anova and p-values of coefficients to choose to the ```best``` explanatory variable.\n",
    "\n",
    "*Neither does Scikit-learn allow for Anova to compare linear model nor does Scikit-learn  compute P values of coefficients of a linear model.* \n",
    "\n",
    "If you would like to do such statistical analysis, you need to use the ```statsmodels``` package. There maybe a workshop on this package in the summer.\n",
    "\n",
    "While Scikit cannot do such analysis, I argue that Anova and p-values are not the most effective ways of comparing linear models. \n",
    "\n",
    "- Anova depends on the order of term by term comparison\n",
    "- p-values of coefficients change depending other coefficients used in the model.\n",
    "\n",
    "## 10.D Overfitting\n",
    "\n",
    "It is important that we choose the most variables to put in the linear model. It is also important we do *overfit*. Overfitting is learning the training set too closely. \n",
    "\n",
    "Any sample population is not representative of the general population. Additionally, most data contain noise. A model that learns the training set to the T can reproduce behaviors that are mostly seen in the sample population.\n",
    "\n",
    "\n",
    "The test error is a heuristic used to measure fidelity to the general population. The test set is another sample of the general population that the model has not seen before. If the model performs poorly on the test set, then it is likely to perform poorly on the general population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.E.1 Model Selection: LASSO\n",
    "\n",
    "LASSO is a reformulation of the linear regression problem that removes coefficients that do not decrease the training error by a relatively large amount.\n",
    "\n",
    "Previously, we had the regression problem as: \n",
    "$$\\min_{\\beta_0 ,\\beta_1 ,\\beta_2 ,\\beta_3,\\beta_4} \\sum_{i=1}^{N}\\left(y_i - \\left(\\beta_0 + \\beta_1 \\text{Total Volume}_i + \\beta_2 \\text{Year}_i + \\beta_3 \\text{Month}_i + \\beta_4 \\text{Type}_i\\right)\\right)^2.$$\n",
    "\n",
    "LASSO reformulates the regression as:\n",
    "\n",
    "$$\\min_{\\beta_0 ,\\beta_1 ,\\beta_2 ,\\beta_3,\\beta_4} \\sum_{i=1}^{N}\\left(y_i - \\left(\\beta_0 + \\beta_1 \\text{Total Volume}_i + \\beta_2 \\text{Year}_i + \\beta_3 \\text{Month}_i + \\beta_4 \\text{Type}_i\\right)\\right)^2 + \\alpha \\sum_{j=1}^{4}|\\beta_j|.$$\n",
    "\n",
    "$\\alpha$ weighs eliminating non-zero coefficients against a decrease in training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.E.2 Building a linear model with ```Lasso```\n",
    "\n",
    "#### I. Initialize linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#initialize Lasso with alpha=0.01, fit_intercept=True, normalize=False\n",
    "# store as lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Fitting linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store avocado_training_set_cleaned[['Total Volume','year_2016','year_2017', 'year_2018']] as\n",
    "# explanatory variable\n",
    "\n",
    "\n",
    "\n",
    "# print explanatory varaible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lasso with avocado_training_set_cleaned and prices_training_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Getting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Interpretting model \n",
    "\n",
    "Our model is now \n",
    "\n",
    "$$\\text{Price} =2.5112 -0.1014\\times\\text{Total Demand} + 0.1298 \\times \\text{year_2017}.$$\n",
    "\n",
    "Note that our regression model without regularization was\n",
    "\n",
    "$$\\text{Price} = 2.5260 -0.1040 \\times \\text{Log(Total Demand)}-0.0106 \\times \\text{year_2016} + 0.1767\\times\\text{year_2017}+ 0.04261\\times\\text{year_2018}.$$\n",
    "\n",
    "We can interpret this reduction in our model as:\n",
    "\n",
    "\"*given $\\alpha=0.01$, $\\text{year_2016}$ and $\\text{year_2018}$ did not provide a large enough reduction in the training error to retain*.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Plotting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# predicted price with .predict(explanatory_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# create index of variables that are 2017 and not 2017 \n",
    "year_2017 = avocado_training_set_cleaned['year_2017'].astype(bool)\n",
    "year_not_2017 = (year_2017 == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "list_years = [year_not_2017,year_2017]\n",
    "years = [\"not 2017\",\"2017\"]\n",
    "colors = ['r','b']\n",
    "legend_name_scatter= []\n",
    "legend_name_line= []\n",
    "for i in range(len(list_years)):\n",
    "    indices = list_years[i]\n",
    "    # get data\n",
    "    training_total_volume_year_ = avocado_training_set_cleaned.loc[indices,\"Total Volume\"]\n",
    "    training_price_year_ = prices_training_set[indices]\n",
    "    predicted_training_prices_year = predict_training_prices_[indices]\n",
    "\n",
    "\n",
    "    # scatter plot of the demand \n",
    "    plt.scatter(training_total_volume_year_, training_price_year_ , s=5,\n",
    "                alpha=0.3,color=colors[i])\n",
    "    # plot of linear model \n",
    "    plt.plot(training_total_volume_year_, predicted_training_prices_year,c=colors[i])\n",
    "    legend_name_scatter.extend(['data for ' + years[i]])\n",
    "    legend_name_line.extend(['predicted model for ' + years[i]])\n",
    "legend_name_line.extend(legend_name_scatter)\n",
    "plt.xlabel('log(Total Volume)')\n",
    "plt.ylabel('Price')\n",
    "plt.legend('')\n",
    "plt.legend(legend_name_line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.E.3 Exercise: Building a linear model with ```Lasso``` with a large $\\alpha$\n",
    "\n",
    "Following the instructions above and the explanatory variables, ```['Total Volume','year_2016','year_2017', 'year_2018']```, build a linear model with Lasso and set ```alpha=1.0```. \n",
    "\n",
    "Print the coefficients of the linear model and compare these coefficients to those of the linear model without regularization:\n",
    "\n",
    "$$\\text{Price} = 2.5260 -0.1040 \\times \\text{Log(Total Demand)}-0.0106 \\times \\text{2016_year} + 0.1767\\times\\text{2017_year}+ 0.04261\\times\\text{2018_year}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1.0, fit_intercept=True, normalize=False)\n",
    "explanatory_data= avocado_training_set_cleaned[['Total Volume','year_2016',\n",
    "                                                'year_2017', 'year_2018']]\n",
    "\n",
    "\n",
    "# enter solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.E.4 Exercise: Building a linear model with ```Lasso``` with a small $\\alpha$\n",
    "\n",
    "Following the instructions above and the explanatory variables, ```['Total Volume','year_2016','year_2017', 'year_2018']```, build a linear model with Lasso and set ```alpha= 1e-6```. \n",
    "\n",
    "Print the coefficients of the linear model and compare these coefficients to those of the linear model without regularization:\n",
    "\n",
    "$$\\text{Price} = 2.5260 -0.1040 \\times \\text{Log(Total Demand)}-0.0106 \\times \\text{2016_year} + 0.1767\\times\\text{2017_year}+ 0.04261\\times\\text{2018_year}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1e-6, fit_intercept=True, normalize=False)\n",
    "explanatory_data= avocado_training_set_cleaned[['Total Volume','year_2016',\n",
    "                                                'year_2017', 'year_2018']]\n",
    "# enter solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.F Choosing the best $\\alpha$\n",
    "\n",
    "Varying $\\alpha$ drastically affect the model we learn. Is there an alpha that most help us find the most effective model?\n",
    "\n",
    "## 10.F.1 Cross Validation\n",
    "\n",
    "\n",
    "Ideally, we would like to find an $\\alpha$ that minimizes model error in the general population.\n",
    "\n",
    "<img src=\"images/10_alpha_model_error.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "$\\alpha$ is a hyperparameter of machine learning model. A hyperparameter is value that must be set before learning begins.\n",
    "\n",
    "In machine learning, hyperparameters are learnt using *cross validation*. \n",
    "\n",
    "Cross validation is process by which a dataset into smaller datasets and hyperparameters that minimizes the average error on the sub-datasets are learn.\n",
    "\n",
    "\n",
    "## 10.F.2 Process of Cross Validation\n",
    "\n",
    "1) Divide the dataset into *folds*. Folds are smaller datasets of a larger dataset.\n",
    "\n",
    "<img src=\"images/10_folds.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "2) Set the hyperparameter to an intelligent guess.\n",
    "\n",
    "3) A fold is then removed and, given the hyperparameter above, the model is trained on the remaining folds.\n",
    "\n",
    "<img src=\"images/10_cross_validation.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "4) Repeat step 3 repeat until each fold as been removed at least once.\n",
    "\n",
    "5) Vary the hyperparameter.\n",
    "\n",
    "6) Repeat Steps 3-4.\n",
    "\n",
    "7) Repeat steps 5-6 until varying hyperparameter always leads to an increase in model error.\n",
    "\n",
    "**Note: If you divide your dataset into $k$ folds, the cross validation is called *k-fold cross validation*.**\n",
    "\n",
    "**If you divide your dataset so that each fold contains one data point, the cross validation is called *leave-one-out cross validation*.**\n",
    "\n",
    "\n",
    "## 10.F.3 Cross Validation in Scikit-Learn\n",
    "\n",
    "To do a hyperparameter search using the cross validation, we use the ```GridSearchCV``` class. It takes as argument:\n",
    "- the estimator object you are using\n",
    "- the hyperparameter points to check\n",
    "- the scoring method used, for example mean squared error\n",
    "- the number of folds\n",
    "\n",
    "#### I. Initialize ```GridSearchCV```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# initialize new Lasso class\n",
    "\n",
    "# set parameter grid, {'alpha' : [1.0,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9]}\n",
    "\n",
    "# initalize GridSearchCV with lasso, param_grid, scoring='neg_mean_squared_error',cv=10) as lasso_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II. Learn best hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_data= avocado_training_set_cleaned[['Total Volume','year_2016','year_2017', 'year_2018']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit lasso_cv with explanatory_data and prices_training_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print lasso_cv.best_params_\n",
    "\n",
    "\n",
    "# print best_params_['alpha'] and store as best_alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Re-train Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# initalize lasso with alpha=best_alpha, fit_intercept=True, normalize=False)\n",
    "\n",
    "\n",
    "# fit lasso with explanatory_data and prices_training_set)\n",
    "\n",
    "\n",
    "# print coef_ and intercept_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the choice of variables ```'Total Volume','year_2016', 'year_2017', 'year_2018'```, ```LASSO``` finds the best model to be\n",
    "\n",
    "$$\\text{Price} = 2.6749 -0.122 \\times \\text{Log(Total Demand)}-0.0027 \\times \\text{2016_year} + 0.1988\\times\\text{2017_year}+  0.0481\\times\\text{2018_year}.$$\n",
    "\n",
    "This is roughly the same as the linear model without regularization:\n",
    "\n",
    "$$\\text{Price} = 2.5260 -0.1040 \\times \\text{Log(Total Demand)}-0.0106 \\times \\text{2016_year} + 0.1767\\times\\text{2017_year}+ 0.04261\\times\\text{2018_year}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.F.4 Exercise: Cross Validation with LassoCV.\n",
    "\n",
    "Using Lasso with cross validation is so common that scikit-learn created a class for it! It is alot simplier than using ```GridSearchCV```. \n",
    "\n",
    "With the explanatory variables, ['Total Volume','year_2016','year_2017', 'year_2018'], and response variable, ```prices_training_set```, follow the steps below use ```LassoCV```.\n",
    "\n",
    "\n",
    "#### I. Initialize\n",
    "\n",
    "First, you need to initialize ```LassoCV(alphas,cv)``` as ```lasso_cv```. It takes arguments:\n",
    "\n",
    "- ```alphas``` - a list of alphas\n",
    "- ```cv``` - the number of folds.\n",
    "\n",
    "I set the values of arguments below. ```LassoCV``` computes the mean squared error on the remaining folds.\n",
    "\n",
    "#### II. Fitting \n",
    "\n",
    "Next, you need to fit your instance of ```LassoCV``` to the training data. Use ```explanatory_data``` and ```training_response``` to fit your model.\n",
    "\n",
    "#### III. Get best alpha\n",
    "\n",
    "After getting the best fitting your instance, ```LassoCV``` stores the best alpha as ```alpha_```. Get the best alpha and store it as ```best_alpha.``` Print the best alpha.\n",
    "\n",
    "#### IV. Get model parameters\n",
    "\n",
    "```LassoCV``` also refits ```Lasso``` to the model explanatory data using the best alpha found above. It stores the coefficients, ```coef_``` and ```intercept_```, of Lasso with best alpha. Print these coefficients and interpret associated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanatory variables\n",
    "explanatory_data= avocado_training_set_cleaned[['Total Volume','year_2016',\n",
    "                                                'year_2017', 'year_2018']]\n",
    "\n",
    "training_response = prices_training_set.values.ravel()\n",
    "# reformat prices \n",
    "# load LassoCV\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "# set LassoCV arguments\n",
    "alpha_list = np.logspace(0,-9,num=10) # list of alphas\n",
    "k = 10 # number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter solution for I here\n",
    "# I) initialize LassoCV(alphas=alpha_list,cv=k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter solution for II here\n",
    "# II) fit LassoCV with explanatory_data and training_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter solution for III here\n",
    "# III) get best alpha, lasso_cv.alpha_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter solution for IV here\n",
    "# IV) get coefficent, lasso_cv.coef_, and intercept, lasso_cv.intercept_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.F.5 Exercise: Cross Validation with LassoCV.\n",
    "\n",
    "Repeat the steps in the exercise above to find the best alpha with the explanatory variables, ['Total Volume','type_organic','year_2016','year_2017', 'year_2018'], and response variable, ```prices_training_set```.\n",
    "\n",
    "Train ```Lasso``` with the best alpha found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanatory variables\n",
    "explanatory_data= avocado_training_set_cleaned[['Total Volume','type_organic','year_2016',\n",
    "                                                'year_2017', 'year_2018']]\n",
    "\n",
    "training_response = prices_training_set.values.ravel()\n",
    "# reformat prices \n",
    "# load LassoCV\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "# set LassoCV arguments\n",
    "\n",
    "import numpy as np\n",
    "alpha_list=np.logspace(0,-9,num=20)\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter solution here to train LassoCV\n",
    "\n",
    "# I) initialize LassoCV\n",
    "\n",
    "\n",
    "# II) fit LassoCV\n",
    "\n",
    "\n",
    "# III) get best alpha\n",
    "\n",
    "\n",
    "# IV) get coefficents and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the choice of variables ```'Total Volume','type_organic','year_2016', 'year_2017', 'year_2018',``` LASSO finds the best model to be```LASSO``` finds the best model to be\n",
    "\n",
    "$$\\text{Price} = 1.7465 -0.0479 \\times \\text{Log(Total Demand)} + 0.3203\\times\\text{type_organic}-0.0272 \\times \\text{year_2016} +0.1511\\times\\text{year_2017}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
